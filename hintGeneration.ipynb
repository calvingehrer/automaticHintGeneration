{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hintGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cxaEE5DFfr_v",
        "WckjgVs-fZto",
        "s5rwSDisuN9s",
        "du5Un8VufnlG",
        "_lhmuuUNQ__8",
        "0Yf6Fi_34ye5",
        "0LORwHLog-t_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calvingehrer/automaticHintGeneration/blob/main/hintGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call Functions\n"
      ],
      "metadata": {
        "id": "cxaEE5DFfr_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ZDHtbAkJJ"
      },
      "source": [
        "**Call hint function for all questions where answer is Location**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kHh8TAEt6OS"
      },
      "source": [
        "for index, row in location_df.iterrows():\n",
        "  location = row[\"Answer\"]\n",
        "  questionLocation = row[\"Question\"]\n",
        "  with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionLocation + \" (\" + location + \")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "  #get Wikidata ID for SPARQL-Query\n",
        "  locationQitem = getQitemOfName(location)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsLocation(locationQitem)\n",
        "\n",
        "  #getFinalHints\n",
        "  listHintsLocation = getFormatedHintsLocation(queryResults_df)    \n",
        "\n",
        "  #write hints in file in random order\n",
        "  writeLocationHintsInFile(listHintsLocation)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzskrP0AtKs"
      },
      "source": [
        "**Call hint function for all questions where answer is Person**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__QLqKMAX51"
      },
      "source": [
        "\n",
        "for index, row in person_df.iterrows():\n",
        "  person = row[\"Answer\"]\n",
        "  questionPerson = row[\"Question\"]\n",
        "\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionPerson + \" (\"+ person +\")\\n\\n\")\n",
        "    writefile.close()\n",
        "  \n",
        "  #get Wikipedia ID for SPARQL-Query\n",
        "  personQitem = getQitemOfName(person)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsPerson(personQitem)\n",
        "  \n",
        "  #get Final Hints\n",
        "  listHintsPerson = getFormatedHintsPerson(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writePersonHintsInFile(listHintsPerson)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsplvG5DUJn"
      },
      "source": [
        "**Call hint function for all question where answer is Year**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5x7mjMDY-b"
      },
      "source": [
        "for index, row in year_df.iterrows():\n",
        "  year = row[\"Answer\"]\n",
        "  questionYear = row[\"Question\"]\n",
        "  hintsYear(row[\"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load File"
      ],
      "metadata": {
        "id": "WckjgVs-fZto"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8U-ZpsRss_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.ExcelFile(\"./testSet.xlsx\").parse(\"Sheet1\")\n",
        "x = []\n",
        "x.append(df[\"Answer\"])\n",
        "\n",
        "dataPerson = []\n",
        "dataYear = []\n",
        "dataLocation = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if(row[\"Category\"] == \"Person\"):\n",
        "    dataPerson.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Year\"):\n",
        "    dataYear.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Location\"):\n",
        "    dataLocation.append([row[\"Question\"], row[\"Answer\"]])\n",
        "\n",
        "person_df = pd.DataFrame(dataPerson, columns=[\"Question\", \"Answer\"])\n",
        "year_df = pd.DataFrame(dataYear, columns=[\"Question\", \"Answer\"])\n",
        "location_df = pd.DataFrame(dataLocation, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rwSDisuN9s"
      },
      "source": [
        "# All imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DepwTLww00"
      },
      "source": [
        "!pip install sparqlwrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Commonists/pageview-api.git"
      ],
      "metadata": {
        "id": "Ly74i7kOgemD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "LMeyegyfgdd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "_KjIV0guhBIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpv1aim8jLJ"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7j16mqC9L83"
      },
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "n07hKh23g9dG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag"
      ],
      "metadata": {
        "id": "PyfC4F2Ya6sh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "pQIQvzqoaqbS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQ0BQ8dnv50"
      },
      "source": [
        "import wikipedia"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "-6NVlM8dhEdI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3KDOw-yx4JR"
      },
      "source": [
        "import pageviewapi"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT41X2h85r7j"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5Un8VufnlG"
      },
      "source": [
        "# Functions for reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8t6UzdAaKj"
      },
      "source": [
        "#get Wikidata ID\n",
        "def getQitemOfName(entity):\n",
        "  \n",
        "  searched = \"'\"+entity+\"'\"\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?item ?itemLabel WHERE {\n",
        "    ?item rdfs:label '''+searched+'''@en.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "\n",
        "  }\n",
        "  ''')\n",
        "  \n",
        "  sparql.setReturnFormat(JSON)\n",
        "  entities = sparql.query().convert()\n",
        "  entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "  words = entities_df[[\"item.value\"]].iloc[0].str.split(\"/\")[0]\n",
        "  return words[-1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tNPxs6Cp8f"
      },
      "source": [
        "#get name of entity from Wikidata ID\n",
        "\n",
        "def getLabelOfQitems(entities):\n",
        "\n",
        "  finalLabels = []\n",
        "  for item in entities:\n",
        "    #searched = \"'\"+item+\"'\"\n",
        "    sparql.setQuery('''\n",
        "    SELECT * WHERE {\n",
        "        wd:'''+item+ ''' rdfs:label ?label .\n",
        "        FILTER (langMatches( lang(?label), \"EN\" ) )\n",
        "    } LIMIT 1\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "    label = entities_df[\"label.value\"].iloc[0]\n",
        "    finalLabels.append(label)\n",
        "\n",
        "  return finalLabels  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0zkvbXxRpU"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlinks >= 1000 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinks(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  listWithBacklink500 = []\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for val in entities:\n",
        "    #print(row[\"itemLabel.value\"])\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": val,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[val] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      listWithBacklink500.append(val) \n",
        "\n",
        "  if (len(listWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return listWithBacklink500\n",
        "\n",
        "  else:\n",
        "    listMostBacklinks = []\n",
        "    sortedDict = dict(sorted(dictWithAllEntries.items(), key=lambda item:item[1]))\n",
        "\n",
        "    for k, v in sortedDict.items():\n",
        "      listMostBacklinks.append(k)\n",
        "    \n",
        "    numberToPass = len(listMostBacklinks)//3\n",
        "        \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0fPfmxUZSW"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlink == 500 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinksDict(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  dictWithBacklink500 = {}\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for key, val in entities.items():\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": key,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[key] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "    except KeyError:\n",
        "      print(\"Key Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      dictWithBacklink500.update({key: val}) \n",
        "\n",
        "  if (len(dictWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return dictWithBacklink500\n",
        "\n",
        "    #print(listMostBacklinks[-numberToPass:])\n",
        "\n",
        "    \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4SZXwl1j7d"
      },
      "source": [
        "#get avg pageViews of entities\n",
        "\n",
        "def getPageViewsList(listEntities):\n",
        "  avgDict = {}\n",
        "  \n",
        "  for val in listEntities:\n",
        "    try:\n",
        "      resp = pageviewapi.per_article('en.wikipedia', val, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "    #print(resp)\n",
        "      avgViews = 0\n",
        "      count = 0\n",
        "      for i in resp.get(\"items\"):\n",
        "        #print(i.get(\"views\"))\n",
        "        avgViews += i.get(\"views\")\n",
        "        count += 1\n",
        "      avgDict.update({val: avgViews//count})\n",
        "    except:\n",
        "      print(\"Page not found to check pageViews for page: \" + val)\n",
        "      \n",
        "  #return (max(avgDict, key=avgDict.get))\n",
        "  return avgDict\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhL5jdCN-NdY"
      },
      "source": [
        "#get avg pageViews of entity\n",
        "import pageviewapi\n",
        "\n",
        "\n",
        "def getPageViews(entity):\n",
        "  avgDict = {}\n",
        "  \n",
        "  try:\n",
        "    resp = pageviewapi.per_article('en.wikipedia', entity, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "\n",
        "    avgViews = 0\n",
        "    count = 0\n",
        "    for i in resp.get(\"items\"):\n",
        "      #print(i.get(\"views\"))\n",
        "      avgViews += i.get(\"views\")\n",
        "      count += 1\n",
        "\n",
        "    pageViews = avgViews//count\n",
        "    #avgDict.update({val: avgViews//count})\n",
        "\n",
        "    #return (max(avgDict, key=avgDict.get))\n",
        "    return pageViews\n",
        "  #print(resp)\n",
        "  except:\n",
        "    print(\"Page not found to check pageViews for page: \" + entity)\n",
        "    return -1\n",
        "    \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if entity is a person\n",
        "#return list of persons\n",
        "\n",
        "def checkIfHuman(entity):\n",
        "\n",
        "  #listHumans = []\n",
        "\n",
        "  #for test in entity:\n",
        "    #print(item)\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "\n",
        "  #print(searched)\n",
        "\n",
        "  try:\n",
        "\n",
        "    sparql.setQuery('''\n",
        "        SELECT distinct ?item ?itemLabel \n",
        "        WHERE{\n",
        "          ?item ?label '''+searched+'''.  \n",
        "          ?item wdt:P31 wd:Q5.\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\t\n",
        "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
        "        }\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "\n",
        "    #print(entities)\n",
        "    #print(entities_df[\"itemLabel.value\"].item())\n",
        "\n",
        "    #for index, row in entities_df.iterrows():\n",
        "    return entities_df[\"itemLabel.value\"].item()\n",
        "    \n",
        "    \n",
        "  except:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "QHdp21OCwtgj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJJtksOgQYk"
      },
      "source": [
        "#if parantheses in entity -> remove\n",
        "\n",
        "def removeParanthesesDict(inputDict):\n",
        "\n",
        "  newDict = {}\n",
        "  for key, val in inputDict.items():\n",
        "    if ('(' in key):\n",
        "      newElem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", key).rstrip()\n",
        "      newDict.update({newElem: val})\n",
        "    else:\n",
        "      newDict.update({key: val})\n",
        "\n",
        "  return newDict"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhmuuUNQ__8"
      },
      "source": [
        "#Hints if answer is a Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-T2fLcTRD54"
      },
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsLocation(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P35 wdt:P30 wdt:P36 wdt:P610 wdt:P1082 wdt:P421 wdt:P38 wdt:P17 wdt:P1376 wdt:P131 wdt:P6 wdt:P1830 wdt:P47 wdt:P206 wdt:P37 wdt:P463 wdt:793}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsLocation = sparql.query().convert()\n",
        "\n",
        "  hintsLocation_df = pd.json_normalize(hintsLocation['results']['bindings'])\n",
        "\n",
        "  return hintsLocation_df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1A0iK-sery"
      },
      "source": [
        "#create template based hints\n",
        "\n",
        "def getFormatedHintsLocation(queryResults_df):\n",
        "  timeZone = []\n",
        "  isLocatedIn = []\n",
        "  ownerOf = []\n",
        "  shareBorder = []\n",
        "  bodyOfWater = []\n",
        "  languages = []\n",
        "  memberOf = []\n",
        "  significantEvents = []\n",
        "\n",
        "  listHintsLocation = []\n",
        "\n",
        "  searchString = \"http://www.wikidata.org/prop/direct/\"\n",
        "  \n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(not location in row[\"itemLabel.value\"]):\n",
        "      if(row[\"property.value\"] == searchString + \"P35\"):\n",
        "        listHintsLocation.append(\"Head of state of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P30\"):\n",
        "        listHintsLocation.append(\"The searched location is on continent \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P36\"):\n",
        "        listHintsLocation.append(\"The capital of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"\"):\n",
        "        listHintsLocation.append(\"The highest point in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1082\"):\n",
        "        listHintsLocation.append(\"The searched location has a population of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P38\"):\n",
        "        listHintsLocation.append(\"Currency in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P17\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          listHintsLocation.append(\"The searched location is in country \"+ row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1376\"):\n",
        "        listHintsLocation.append(\"The searched city is capital of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P131\"):\n",
        "        listHintsLocation.append(\"The searched location is located in \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P6\"):\n",
        "        listHintsLocation.append(\"Head of government of searched location is \" + row[\"itemLabel.value\"])\n",
        "      \n",
        "      #if get multiple values save all in list and pick one afterwards\n",
        "      elif(row[\"property.value\"] == searchString + \"P1830\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          ownerOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P47\"):\n",
        "        shareBorder.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P206\"):\n",
        "        bodyOfWater.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P37\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          languages.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P421\"):\n",
        "        timeZone.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P463\"):\n",
        "        memberOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P793\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          significantEvents.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #getting more than one result with one query, pick one for output\n",
        "  if(len(ownerOf) > 0):\n",
        "    listHintsLocation.append(\"The searched location is owner of \" + ownerOf[0])\n",
        "  if(len(shareBorder) > 0):\n",
        "    listHintsLocation.append(\"The searched location shares border with \" + shareBorder[0])\n",
        "  if(len(bodyOfWater) > 0):\n",
        "    listHintsLocation.append(\"The next body of water of searched location is \" + bodyOfWater[0])\n",
        "  if(len(memberOf) > 0):\n",
        "    listBacklinks = getBacklinks(memberOf)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"Searched location is member of \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "  if(len(significantEvents) > 0):\n",
        "    listBacklinks = getBacklinks(significantEvents)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"A significant event happened in this location was \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "\n",
        "  #if getting more than one language concatenate all\n",
        "  commaCounter = len(languages)-1\n",
        "  stringLanguages = \"\"\n",
        "  for language in languages:\n",
        "    if(commaCounter > 0):\n",
        "      stringLanguages += language + \", \"\n",
        "      commaCounter -= 1\n",
        "    else:\n",
        "      stringLanguages += language\n",
        "  if(stringLanguages != \"\"):\n",
        "    listHintsLocation.append(\"Spoken language(s) in searched location is/are \" + stringLanguages)\n",
        "  if(len(timeZone) > 0):\n",
        "    listHintsLocation.append(\"The searched location is in time Zone \" + timeZone[0])\n",
        "\n",
        "  \n",
        "  return listHintsLocation\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65hUdVTRbXU"
      },
      "source": [
        "def writeLocationHintsInFile(listHintsLocation):\n",
        "  randomHintsList = random.sample(listHintsLocation, len(listHintsLocation))\n",
        "  \n",
        "  for item in randomHintsList:\n",
        "    with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "      writefile.write(item + \"\\n\")\n",
        "      writefile.close()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yf6Fi_34ye5"
      },
      "source": [
        "#Hints if answer is a Person"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsPerson(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P21 wdt:P27 wdt:P569 wdt:P19 wdt:P1971 wdt:P106 wdt:P1340 wdt:P1884 wdt:P2048 wdt:P39 wdt:P69 wdt:P512 wdt:P102 wdt:P3602 wdt:P800 wdt:P166 wdt:P3373 wdt:P1412 wdt:P413 wdt:P118 wdt:P54}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsPerson = sparql.query().convert()\n",
        "\n",
        "  hintsPerson_df = pd.json_normalize(hintsPerson['results']['bindings'])\n",
        "  \n",
        "  return hintsPerson_df"
      ],
      "metadata": {
        "id": "Lm8zYu5C3xSO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1ZtKwL5hM-"
      },
      "source": [
        "#create temolate based hints\n",
        "\n",
        "def getFormatedHintsPerson(queryResults_df):\n",
        "\n",
        "  listOccupation = [\"Searched person was occuupied as \"]\n",
        "  listPositionHeld = [\"Searched person held the position(s) \"]\n",
        "  listEducatedAt = [\"Searched person was educated at \"]\n",
        "  listAcademicDegrees = [\"Searched person has academic degrees \"]\n",
        "  listPoliticParty = [\"Searched was/is member of politic party \"]\n",
        "  listCandidacyElection = [\"Searched person was candidacy at election(s) \"]\n",
        "  listNotableWork = [\"Searched person did notable work \"]\n",
        "  listAwardsReceived = [\"Searched person received awards \"]\n",
        "  listLanguages = [\"Searched person speaks language(s) \"]\n",
        "  listPlayedPositions = [\"Searched person played position \"]\n",
        "  listLeague = [\"Searched person played in leagues \"]\n",
        "  listTeams = [\"Searched person played in teams \"]\n",
        "  listCitizenship = [\"Searched person has following citizenships \"]\n",
        "  listSiblings = []\n",
        "  listHair = []\n",
        "\n",
        "  listHintsPerson = []\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P21\"):\n",
        "      listHintsPerson.append(\"Gender of searched person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P569\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"][:4])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P19\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1971\"):\n",
        "      listHintsPerson.append(\"Number of children of seared person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1340\"):\n",
        "      listHintsPerson.append(\"Searched person has \" + row[\"itemLabel.value\"] + \" eyes\")    \n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P2048\"):\n",
        "      listHintsPerson.append(\"Searched persons height is \" + row[\"itemLabel.value\"])    \n",
        "\n",
        "    \n",
        "    #if get multiple values save all in list and pick one afterwards\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P27\"):\n",
        "      listCitizenship.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P106\"):\n",
        "      listOccupation.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P39\"):\n",
        "      listPositionHeld.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P69\"):\n",
        "      listEducatedAt.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P512\"):\n",
        "      listAcademicDegrees.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P102\"):\n",
        "      listPoliticParty.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3602\"):\n",
        "      listCandidacyElection.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P800\"):\n",
        "      listNotableWork.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P166\"):\n",
        "      listAwardsReceived.append(row[\"itemLabel.value\"])\n",
        "\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1412\"):\n",
        "      listLanguages.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P413\"):\n",
        "      listPlayedPositions.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P118\"):\n",
        "      listLeague.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P54\"):\n",
        "      listTeams.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3373\"):\n",
        "      listSiblings.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1884\"):\n",
        "      listHair.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #if too many results could be return check\n",
        "  #if more thane 3 -> cut\n",
        "  listResultsCut = [listPositionHeld, listAcademicDegrees, listPoliticParty, \n",
        "                 listCandidacyElection, listNotableWork, listLanguages, \n",
        "                 listPlayedPositions, listLeague, listTeams, listCitizenship, listAwardsReceived, listEducatedAt, listOccupation]\n",
        "  for singleList in listResultsCut:\n",
        "    cutResults(singleList, listHintsPerson)\n",
        "\n",
        "  #just get one hair color\n",
        "  if(len(listHair) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + listHair[0])\n",
        "\n",
        "  #get number of siblings\n",
        "  if(len(listSiblings) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + str(len(listSiblings)) + \" siblings\")  \n",
        "  \n",
        "  return listHintsPerson"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if more results than 3 -> print 3 results + \"and x others\"\n",
        "\n",
        "def cutResults(listResults, listHintsPerson):\n",
        "  \n",
        "  numResults = len(listResults)-1\n",
        "  resultsLeft = numResults-3\n",
        "\n",
        "  if (len(listResults) > 1):\n",
        "    if(resultsLeft > 0):\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults[0:4])+ \" and \" + str(resultsLeft) + \" others\")\n",
        "    else:\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults))"
      ],
      "metadata": {
        "id": "KDP4yoRpB_Sq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOq3XcwIr2a"
      },
      "source": [
        "#concatenate multiple results with comma\n",
        "\n",
        "def getConcatenationMultipleResults(resultList):\n",
        "  concatenation = \"\"\n",
        "  commaCounter = len(resultList)-1\n",
        "  for item in resultList:\n",
        "    if(commaCounter == len(resultList)-1):\n",
        "      concatenation += item\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter > 0):\n",
        "      concatenation += item + \", \"\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter == 0):\n",
        "      concatenation += item\n",
        "  \n",
        "  return concatenation\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74PSzscAzl8"
      },
      "source": [
        "def writePersonHintsInFile(listHintsPerson):\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    #writefile.write(\"\\n\\n---Hints for Person \" + person + \"---\\n\\n\")\n",
        "    randomHintsList = random.sample(listHintsPerson, len(listHintsPerson))\n",
        "    for item in randomHintsList:\n",
        "      writefile.write(item + \"\\n\")\n",
        "    writefile.close()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LORwHLog-t_"
      },
      "source": [
        "# Hints if answer is a Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dil5f183hhHJ"
      },
      "source": [
        "**Get all outlinks of wikipedia year page and pass to getBacklinks. Than look for return values on page.content and print line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFplu-7AF1L"
      },
      "source": [
        "def hintsYear(year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageContent = page.content\n",
        "\n",
        "  #get all links from year page\n",
        "  dictPageLinks = getOutlinks2(year)\n",
        "\n",
        "  #get rid of unnesercery links\n",
        "  filteredPageLinks = filterDict(dictPageLinks)\n",
        "\n",
        "  \n",
        "  #get backlinks of all links\n",
        "  dictBacklink = getBacklinksDict(filteredPageLinks)\n",
        "  #first200 = {k: filteredPageLinks[k] for k in list(filteredPageLinks)[:200]}\n",
        "  #dictBacklink = getBacklinksDict(first200)\n",
        "\n",
        "  #remove parantheses of links\n",
        "  dictAdjusted = removeParanthesesDict(dictBacklink)\n",
        "\n",
        "  #get all possible hint (all lines with a entity #backlinks > 1000)\n",
        "  dictPossibleHints = findLineInTextEvent(dictAdjusted, year)\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "    writefile.write(\"Question: \" + questionYear + \"(\"+ str(year) + \")---\\n\")\n",
        "\n",
        "  #check if entity because of which line was found is subject\n",
        "  #if it is not the subject -> drop\n",
        "  dictFinalHints = getFinalHints(dictPossibleHints)\n",
        "\n",
        "  #get rid of number of foundevents at end of line\n",
        "  #need to put number at the and because it is dict and would be overwritten\n",
        "  dictFinalHintsAdjusted = {}\n",
        "  for key, val in dictFinalHints.items():\n",
        "    dictFinalHintsAdjusted.update({key.split('///', 1)[0]: val})  \n",
        "\n",
        "  #returns dict sorted after PageViews\n",
        "  dictSortedPV = sortHintsNumberPVSubject(dictFinalHintsAdjusted)\n",
        "\n",
        "  #get highest PageView\n",
        "  maxPV = list(dictSortedPV.values())[0]\n",
        "\n",
        "  #sort hints PageViews and Simscore combined\n",
        "  sortHintsUtilityScore(questionYear, dictSortedPV, maxPV)\n",
        "\n",
        "  listHumansNames = []\n",
        "\n",
        "  for key, val in dictAdjusted.items():\n",
        "    result = checkIfHuman(key)\n",
        "    if(result != None):\n",
        "      listHumansNames.append(result)\n",
        "\n",
        "  findLineInTextBirthDeath(listHumansNames, year)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dJk5vrhq6M"
      },
      "source": [
        "**Function to find line of most important link in year content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GonOTeikJm"
      },
      "source": [
        "import re\n",
        "\n",
        "def findLineInTextEvent(entitiesEvent, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Events ==|== Births ==\", pageText ))\n",
        "\n",
        "  contentEvents = splitText[1]\n",
        "\n",
        "  foundEvent = 0\n",
        "\n",
        "  dictHintsEntity = {}\n",
        "\n",
        "  for key, val in entitiesEvent.items():\n",
        "    #if (foundEvent == 20):\n",
        "      #break;\n",
        "\n",
        "    for line in contentEvents.split(\"\\n\"):\n",
        "      #if (foundEvent == 20):\n",
        "        #break;\n",
        "\n",
        "      if val in line:\n",
        "        words = line.split()\n",
        "        #counterEntitiesInLine = 0\n",
        "        if(len(words) < 15):\n",
        "          dictHintsEntity.update({line + \"///\" + str(foundEvent): val})\n",
        "          #listHints.append(line)\n",
        "          foundEvent += 1\n",
        "          #break;\n",
        "  return dictHintsEntity"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAM4MxB-8By"
      },
      "source": [
        "def findLineInTextBirthDeath(entitiesHuman, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Births ==|== Deaths ==\", pageText ))\n",
        "\n",
        "\n",
        "  contentBirths = splitText[1]\n",
        "  contentDeaths = splitText[2]\n",
        "\n",
        "  foundBirth = 0\n",
        "  foundDeath = 0\n",
        "\n",
        "  #open file and append results\n",
        "  with open('./resultsBirthDeath.txt', 'a') as writefile:\n",
        "    writefile.write(\"---Birth/Death in year \" + str(year) + \"---\")\n",
        "\n",
        "    for entity in entitiesHuman:\n",
        "      #if (foundBirth == 3 and foundDeath == 3):\n",
        "        #writefile.write(\"\\n\")\n",
        "        #break;\n",
        "\n",
        "      for line in contentBirths.split(\"\\n\"):\n",
        "        #if (foundBirth == 3):\n",
        "          #break;\n",
        "\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Birth: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundBirth +=1\n",
        "          break;\n",
        "\n",
        "      for line in contentDeaths.split(\"\\n\"):\n",
        "        #if (foundDeath == 3):\n",
        "          #break;\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Death: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundDeath += 1\n",
        "          break;\n",
        "    writefile.write(\"\\n\")\n",
        "      "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUy44eeb0EKg"
      },
      "source": [
        "def sortHintsNumberPVSubject(dictHints):\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile: \n",
        "\n",
        "    dictFinalHintsPV = {}\n",
        "\n",
        "    for key, val in dictHints.items():\n",
        "      dictFinalHintsPV.update({key: getPageViews(val)})\n",
        "\n",
        "    dictFinalHintsSortedPV = dict(sorted(dictFinalHintsPV.items(), key=lambda item: item[1], reverse=True))\n",
        "    \"\"\"writefile.write(\"\\n--Final Hints sorted #PV---\\n\")\n",
        "    for key, val in dictFinalHintsSortedPV.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" PageViews)\\n\")\"\"\"\n",
        "\n",
        "  return dictFinalHintsSortedPV\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YpAjm5B6CM"
      },
      "source": [
        "\n",
        "\n",
        "def sortHintsUtilityScore(question, dictFinalHintsSortedPV, maxPV):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  question_embedding = model.encode(question)\n",
        "\n",
        "  alpha = 0.5\n",
        "\n",
        "  #sentence_embeddings = model.encode(hints)\n",
        "\n",
        "  dictHintsBERT = {}\n",
        "\n",
        "  for key, val in dictFinalHintsSortedPV.items():\n",
        "    sentence_embedding = model.encode(key)\n",
        "    simScore = cosine_similarity([question_embedding], [sentence_embedding]).item()\n",
        "    impScore = alpha * (val/maxPV) + (1-alpha) * simScore\n",
        "    dictHintsBERT.update({key: impScore})\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictHintsBERT = dict(sorted(dictHintsBERT.items(), key=lambda item: item[1], reverse=True))\n",
        "    writefile.write(\"\\n--Final Hints sorted Utility Score---\\n\")\n",
        "    for key, val in dictHintsBERT.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" Utility Score)\\n\")\n",
        "\n",
        "  return dictHintsBERT\n",
        " "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuRQ-FsZdQ_"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBif9Lc8_sg3"
      },
      "source": [
        "def getFinalHints(dictPossibleHints):\n",
        "\n",
        "  nlp = spacy.load(\"en\")\n",
        "  dictFinalHintsSubject = {}\n",
        "  \n",
        "  for key, val in dictPossibleHints.items():\n",
        "    doc = nlp(key)\n",
        "    subject = get_subject_phrase(doc)\n",
        "    if (val in str(subject)):\n",
        "      dictFinalHintsSubject.update({key: val})\n",
        "      #listFinalHints.append(key)\n",
        "\n",
        "  return dictFinalHintsSubject"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUt_0x6jD-x"
      },
      "source": [
        "\n",
        "def getOutlinks2(year):\n",
        "\n",
        "  URL = 'https://en.wikipedia.org/wiki/' + str(year)\n",
        "\n",
        "  # Fetch all the HTML source from the url\n",
        "  response = requests.get(URL)\n",
        "\n",
        "\n",
        "  soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  listTextOfLinks = []\n",
        "  listLinks = []\n",
        "\n",
        "  dictLinkName = {}\n",
        "\n",
        "\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    listTextOfLinks.append(link.get_text())\n",
        "    dictLinkName.update({str(link.get(\"href\")).split('/')[-1].replace('_', ' '): link.get_text()})\n",
        "\n",
        "  #newDict = filterDict(dictLinkName)\n",
        "\n",
        "  #print(len(dictLinkName))\n",
        "  #print(len(newDict))\n",
        "  #print(newDict)\n",
        "\n",
        "  return dictLinkName"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuFgVXomCdf"
      },
      "source": [
        "def filterDict(dictNameLink):\n",
        "\n",
        "  patternYear = r\"[1-3][0-9]{3}\"\n",
        "  patternCentury = r\"[1-2][0-9]\\w+.century\"\n",
        "  patternAd = r\"AD.\\d\\d\\d\\d\"\n",
        "  patternMonth = r\"[a-zA-Z]+\\s\\d+\"\n",
        "  patternMillennium = r\".+millennium\"\n",
        "  patternHashtag = r\"#\"\n",
        "  patternCalendar = r\"calendar\"\n",
        "  patternCategory = r\"category\"\n",
        "  patternList = r\"list\"\n",
        "  patternTemplate = r\"template\"\n",
        "  patternWikipedia = r\"wikipedia\"\n",
        "  patternIndex = r\"index\"\n",
        "  patternHtml = r\"html\"\n",
        "  patternPercent = r\"%\"\n",
        "\n",
        "  newDict = {}\n",
        "\n",
        "  for key, val in dictNameLink.items():\n",
        "    if(not re.match(patternYear, key) and\n",
        "       not re.match(patternCentury, key) and \n",
        "       not re.match(patternAd, key) and \n",
        "       not re.match(patternMonth, key) and\n",
        "       not re.search(patternHashtag, key) and\n",
        "       not re.search(patternCalendar, key, re.IGNORECASE) and\n",
        "       not re.search(patternCategory, key, re.IGNORECASE) and\n",
        "       not re.search(patternList, key, re.IGNORECASE) and\n",
        "       not re.search(patternTemplate, key, re.IGNORECASE) and\n",
        "       not re.search(patternWikipedia, key, re.IGNORECASE) and\n",
        "       not re.search(patternIndex, key, re.IGNORECASE) and\n",
        "       not re.search(patternHtml, key, re.IGNORECASE) and\n",
        "       not re.search(patternPercent, key) and\n",
        "       not re.match(patternMillennium, key) and\n",
        "       not val == \"\" and not val == \" \"):\n",
        "      \n",
        "\n",
        "      newDict.update({key: val})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return newDict\n"
      ],
      "execution_count": 45,
      "outputs": []
    }
  ]
}